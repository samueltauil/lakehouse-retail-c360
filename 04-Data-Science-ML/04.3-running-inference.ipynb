{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33237ebe-301c-4d1e-aa93-a40d3b949fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Churn Prediction Inference - Batch or serverless real-time\n",
    "\n",
    "\n",
    "With AutoML, our best model was automatically saved in our MLFlow registry.\n",
    "\n",
    "All we need to do now is use this model to run Inferences. A simple solution is to share the model name to our Data Engineering team and they'll be able to call this model within the pipeline they maintained. That's what we did in our Spark Declarative Pipelines pipeline!\n",
    "\n",
    "Alternatively, this can be schedule in a separate job. Here is an example to show you how MLFlow can be directly used to retriver the model and run inferences.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=7405609900705693&notebook=%2F04-Data-Science-ML%2F04.3-running-inference&demo_name=lakehouse-retail-c360&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-retail-c360%2F04-Data-Science-ML%2F04.3-running-inference&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89f7ec25-4200-4ff5-93cc-6ee8ccf397da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==3.1.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d261620e-a678-499e-9456-7ea9665d8f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "de590ba1-2f4c-423a-bf00-5ef25a648cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Deploying the model for batch inferences\n",
    "\n",
    "Now that our model is available in the Registry, we can load it to compute our inferences and save them in a table to start building dashboards.\n",
    "\n",
    "We will use MLFlow function to load a pyspark UDF and distribute our inference in the entire cluster. If the data is small, we can also load the model with plain python and use a pandas Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f51a67a-d2a6-47fa-b26e-96bb2eda5100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scaling inferences using Spark \n",
    "We'll first see how it can be loaded as a spark UDF and called directly in a SQL function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c9e6abe-37f2-44cb-83b5-84e45e2ddf8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "model_name = \"dbdemos_customer_churn\"\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "#                                                                                                Alias\n",
    "#                                                                                  Model name       |\n",
    "#                                                                                        |          |\n",
    "predict_churn_udf = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/{catalog}.{db}.{model_name}@prod\", env_manager='virtualenv', result_type='long')\n",
    "# Note: virtualenv will recreate an env from scratch which can take some time, but prevent any version issue. If you're using the same compute as for training, you can remove it to use the local env instead (just install the lib from the requirements.txt file as below)\n",
    "#We can use the function in SQL\n",
    "spark.udf.register(\"predict_churn\", predict_churn_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ccc345b-7902-493e-bf95-50afa3e74b08",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run inferences"
    }
   },
   "outputs": [],
   "source": [
    "columns = predict_churn_udf.metadata.get_input_schema().input_names()\n",
    "spark.table('churn_features').withColumn(\"churn_prediction\", predict_churn_udf(*columns)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b607fc7c-e986-4536-a638-d75d214cef42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pure pandas inference\n",
    "If we have a small dataset, we can also compute our segment using a single node and pandas API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4911c3f-ec74-4956-b0c8-1fa3b159d5f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load the model dependencies from MLFlow registry"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "import mlflow\n",
    "# Use the Unity Catalog model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "# download model requirement from remote registry\n",
    "requirements_path = ModelsArtifactRepository(f\"models:/{catalog}.{db}.dbdemos_customer_churn@prod\").download_artifacts(artifact_path=\"requirements.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0bab3e-cd72-487f-8137-1be34953d516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r $requirements_path\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd7dfe9-f496-4f2d-89bb-564d74e48399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e78bd587-67e5-43d6-b198-b5d4fc0b3f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = \"dbdemos_customer_churn\"\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{catalog}.{db}.{model_name}@prod\")\n",
    "columns = model.metadata.get_input_schema().input_names()\n",
    "df = spark.table('churn_features').select(*columns).limit(10).toPandas()\n",
    "df['churn_prediction'] = model.predict(df)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "fe8e1fb1-b677-4f52-96f3-ed2936adf32c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Realtime model serving with Databricks serverless serving\n",
    "\n",
    "<img style=\"float: right; margin-left: 20px\" width=\"700\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/lakehouse-c360-model-serving.png?raw=true\" />\n",
    "\n",
    "Databricks also provides serverless serving.\n",
    "\n",
    "Click on model Serving, enable realtime serverless and your endpoint will be created, providing serving over REST api within a Click.\n",
    "\n",
    "Databricks Serverless offer autoscaling, including downscaling to zero when you don't have any traffic to offer best-in-class TCO while keeping low-latencies model serving.\n",
    "\n",
    "To deploy your serverless model, open the [Model Serving menu](https://xxxx.cloud.databricks.com/?o=1660015457675682#mlflow/endpoints), and select the model you registered within Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b1362ea-5ddd-4953-b222-0f8f94c59d41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploy the endpoint via databricks sdk client"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "model_endpoint_name = \"dbdemos_customer_churn_endpoint\"\n",
    "last_version = get_last_model_version(f\"{catalog}.{db}.{model_name}\")\n",
    "client = get_deploy_client(\"databricks\")\n",
    "try:\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=model_endpoint_name,\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": f\"dbdemos_customer_churn_endpoint_{last_version}\",\n",
    "                    \"entity_name\": f\"{catalog}.{db}.{model_name}\",\n",
    "                    \"entity_version\": last_version,\n",
    "                    \"workload_size\": \"Small\",\n",
    "                    \"scale_to_zero_enabled\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"Endpoint {catalog}.{db}.{model_endpoint_name} already exists. Skipping creation.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "while client.get_endpoint(model_endpoint_name)['state']['config_update'] == 'IN_PROGRESS':\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01cfb7d3-2dc8-4615-b050-fb65332c4803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = spark.table('churn_features').select(*columns).limit(3).toPandas()\n",
    "#Make it a string to send to the inference endpoint\n",
    "dataset['last_transaction'] = dataset['last_transaction'].astype(str)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15886eb7-927e-4cb5-a5a3-6ed7819a932e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call the REST API deployed using standard python"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import deployments\n",
    "\n",
    "def score_model(dataset):\n",
    "  client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "  payload = {\"dataframe_split\": dataset.to_dict(orient='split')}\n",
    "  predictions = client.predict(endpoint=model_endpoint_name, inputs=payload)\n",
    "  print(predictions)\n",
    "\n",
    "#Deploy your model and uncomment to run your inferences live!\n",
    "score_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bda1c0e1-d2c9-47a3-9acb-6cf51edd0fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Next step: Leverage inferences and automate actions to increase revenue\n",
    "\n",
    "## Automate action to reduce churn based on predictions\n",
    "\n",
    "We now have an end 2 end data pipeline analizing and predicting churn. We can now easily trigger actions to reduce the churn based on our business:\n",
    "\n",
    "- Send targeting email campaign to the customer the most likely to churn\n",
    "- Phone campaign to discuss with our customers and understand what's going\n",
    "- Understand what's wrong with our line of product and fixing it\n",
    "\n",
    "These actions are out of the scope of this demo and simply leverage the Churn prediction field from our ML model.\n",
    "\n",
    "## Track churn impact over the next month and campaign impact\n",
    "\n",
    "Of course, this churn prediction can be re-used in our dashboard to analyse future churn and measure churn reduction. \n",
    "\n",
    "The pipeline created with the Lakehouse will offer a strong ROI: it took us a few hours to setup this pipeline end 2 end and we have potential gain for $129,914 / month!\n",
    "\n",
    "<img width=\"800px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-dbsql-prediction-dashboard.png\">\n",
    "\n",
    "<a dbdemos-dashboard-id=\"churn-prediction\" href='/sql/dashboardsv3/01f0f16ad84a15ef89c46929f7810112'>Open the Churn prediction DBSQL dashboard</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4cbe77b-bf3a-42b5-b733-9bd0938c9c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Reducing churn leveraging Databricks GenAI and LLMs capabilities \n",
    "\n",
    "GenAI provides unique capabilities to improve your customer relationship, providing better services but also better analyzing your churn risk.\n",
    "\n",
    "Databricks provides built-in GenAI capabilities for you to accelerate such GenAI apps deployment. \n",
    "\n",
    "Discover how with the [Agent Tools]($../05-Generative-AI/05.1-Agent-Functions-Creation) Notebook in the new Generative AI section of this demo!\n",
    "\n",
    "[Go back to the introduction]($../00-churn-introduction-lakehouse)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "04.3-running-inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
