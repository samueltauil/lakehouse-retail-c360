{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143e2dc9-76cd-4100-b35f-f5e8f1e06814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d9a2b9-617e-4e0a-a086-648920b67af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f70e05b2-3fbb-422d-83d7-0dfa3f619746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "major, minor = sys.version_info[:2]\n",
    "assert (major, minor) >= (3, 11), f\"This demo expect python version 3.11, but found {major}.{minor}. \\nUse DBR15.4 or above. \\nIf you're on serverless compute, open the 'Environment' menu on the right of your notebook, set it to >=2 and apply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15b12586-3ceb-48ec-a21d-100039b7c374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae2f0b29-cb0a-427a-bca1-01b6da752296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.setup_schema(catalog, db, reset_all_data, volume_name)\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/{volume_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48e1d8c-6be7-43b1-9551-a26479e385fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_model_version(model_full_name):\n",
    "    from mlflow import MlflowClient\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    # Use the MlflowClient to get a list of all versions for the registered model in Unity Catalog\n",
    "    all_versions = mlflow_client.search_model_versions(f\"name='{model_full_name}'\")\n",
    "    # Sort the list of versions by version number and get the latest version\n",
    "    latest_version = max([int(v.version) for v in all_versions])\n",
    "    # Use the MlflowClient to get the latest version of the registered model in Unity Catalog\n",
    "    return mlflow_client.get_model_version(model_full_name, str(latest_version)).version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80d48996-8c7b-44d5-8a7c-3ecca77acabd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, sha1, col, initcap, to_timestamp\n",
    "\n",
    "folder = f\"/Volumes/{catalog}/{db}/{volume_name}\"\n",
    "\n",
    "if reset_all_data or DBDemos.is_any_folder_empty([folder+\"/orders\", folder+\"/users\", folder+\"/events\"]):\n",
    "  #data generation on another notebook to avoid installing libraries (takes a few seconds to setup pip env)\n",
    "  print(f\"Generating data under {folder} , please wait a few sec...\")\n",
    "  path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "  parent_count = path[path.rfind(\"lakehouse-retail-c360\"):].count('/') - 1\n",
    "  prefix = \"./\" if parent_count == 0 else parent_count*\"../\"\n",
    "  prefix = f'{prefix}_resources/'\n",
    "  dbutils.notebook.run(prefix+\"01-load-data\", 600, {\"reset_all_data\": dbutils.widgets.get(\"reset_all_data\")})\n",
    "else:\n",
    "  print(\"data already existing. Run with reset_all_data=true to force a data cleanup for your local demo.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
