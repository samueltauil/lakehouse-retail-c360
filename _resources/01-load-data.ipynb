{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6ebf6e9-6454-49ed-a938-77397dc2fe7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data initialization notebook. \n",
    "Do not run outside of the main notebook. This will automatically be called based on the reste_all widget value to setup the data required for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a95869-aa09-44d3-9274-bfec5fcdd9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a9cef6e-1711-4197-9c17-79eba7a28a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker databricks-sdk==0.40.0 mlflow==2.22.0 cloudpickle==3.0.0 numpy==1.26.4 pandas==2.2.3\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9803fdce-597b-40a8-859e-01591d043678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72acd0c-9102-440c-bf9e-c01de344ddc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d420b76-b991-4a02-8be4-7d8200df6bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "major, minor = sys.version_info[:2]\n",
    "assert (major, minor) >= (3, 11), f\"This demo expect python version 3.11, but found {major}.{minor}. \\nUse DBR15.4 or above. \\nIf you're on serverless compute, open the 'Environment' menu on the right of your notebook, set it to >=2 and apply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16273b1f-df1e-4a99-969d-40713f2b1e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "DBDemos.setup_schema(catalog, db, reset_all_data, volume_name)\n",
    "folder = f\"/Volumes/{catalog}/{db}/{volume_name}\"\n",
    "\n",
    "data_exists = False\n",
    "try:\n",
    "  dbutils.fs.ls(folder)\n",
    "  dbutils.fs.ls(folder+\"/orders\")\n",
    "  dbutils.fs.ls(folder+\"/users\")\n",
    "  dbutils.fs.ls(folder+\"/events\")\n",
    "  dbutils.fs.ls(folder+\"/ml_features\")\n",
    "  data_exists = True\n",
    "  print(\"data already exists\")\n",
    "except Exception as e:\n",
    "  print(f\"folder doesn't exists, generating the data...\")\n",
    "\n",
    "\n",
    "def cleanup_folder(path):\n",
    "  #Cleanup to have something nicer\n",
    "  for f in dbutils.fs.ls(path):\n",
    "    if f.name.startswith('_committed') or f.name.startswith('_started') or f.name.startswith('_SUCCESS') :\n",
    "      dbutils.fs.rm(f.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2325163-5fa6-419d-b6a1-7e0bb168c9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Let's download the data from dbdemos resource repo\n",
    "If this fails, fallback on generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8037f23-9c02-457e-844f-39f51b1473a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_downloaded = False\n",
    "if not data_exists:\n",
    "    try:\n",
    "        DBDemos.download_file_from_git(folder+'/events', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/events\")\n",
    "        DBDemos.download_file_from_git(folder+'/orders', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/orders\")\n",
    "        DBDemos.download_file_from_git(folder+'/users', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/users\")\n",
    "        DBDemos.download_file_from_git(folder+'/ml_features', \"databricks-demos\", \"dbdemos-dataset\", \"/retail/c360/ml_features\")\n",
    "        data_downloaded = True\n",
    "    except Exception as e: \n",
    "        print(f\"Error trying to download the file from the repo: {str(e)}. Will generate the data instead...\")    \n",
    "else:\n",
    "    data_downloaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56c44d1e-339f-4705-a4b0-6f723bc11ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#As we need a model in the SDP pipeline and the model depends of the SDP pipeline too, let's build an empty one.\n",
    "#This wouldn't make sense in a real-world system where we'd have 2 jobs / pipeline (1 for ingestion, and 1 to build the model / run inferences)\n",
    "import random\n",
    "import mlflow\n",
    "from  mlflow.models.signature import ModelSignature\n",
    "import cloudpickle\n",
    "from unittest import mock\n",
    "import numpy as np\n",
    "\n",
    "# define a custom model\n",
    "class ChurnEmptyModel(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input):\n",
    "        import random\n",
    "        return model_input['user_id'].apply(lambda x: random.randint(0, 1)).astype('int32')\n",
    "\n",
    "#Enable Unity Catalog with mlflow registry\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "model_name = \"dbdemos_customer_churn\"\n",
    "#Only register empty model if model doesn't exist yet\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "try:\n",
    "  latest_model = client.get_model_version_by_alias(f\"{catalog}.{db}.{model_name}\", \"prod\")\n",
    "except Exception as e:\n",
    "\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e) or \"NOT_FOUND\" in str(e):\n",
    "        print(\"Model doesn't exist - saving an empty one\")\n",
    "        # setup the experiment folder\n",
    "        DBDemos.init_experiment_for_batch(\"lakehouse-retail-c360\", \"customer_churn_mock\")\n",
    "        # save the model\n",
    "        churn_model = ChurnEmptyModel()\n",
    "        import pandas as pd\n",
    "        signature = ModelSignature.from_dict({'inputs': '[{\"name\": \"user_id\", \"type\": \"string\"}, {\"name\": \"age_group\", \"type\": \"long\"}, {\"name\": \"canal\", \"type\": \"string\"}, {\"name\": \"country\", \"type\": \"string\"}, {\"name\": \"gender\", \"type\": \"long\"}, {\"name\": \"order_count\", \"type\": \"long\"}, {\"name\": \"total_amount\", \"type\": \"long\"}, {\"name\": \"total_item\", \"type\": \"long\"}, {\"name\": \"last_transaction\", \"type\": \"datetime\"}, {\"name\": \"platform\", \"type\": \"string\"}, {\"name\": \"event_count\", \"type\": \"long\"}, {\"name\": \"session_count\", \"type\": \"long\"}, {\"name\": \"days_since_creation\", \"type\": \"long\"}, {\"name\": \"days_since_last_activity\", \"type\": \"long\"}, {\"name\": \"days_last_event\", \"type\": \"long\"}]',\n",
    "'outputs': '[{\"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"int32\", \"shape\": [-1]}}]'})\n",
    "        #Temporary pin python to 3.11.10\n",
    "        with mlflow.start_run(run_name=\"mockup_model\") as run, mock.patch(\"mlflow.utils.environment.PYTHON_VERSION\", DBDemos.get_python_version_mlflow()):\n",
    "            model_info = mlflow.pyfunc.log_model(artifact_path=\"model\", python_model=churn_model, signature=signature, pip_requirements=['mlflow=='+mlflow.__version__, 'pandas=='+pd.__version__, 'numpy=='+np.__version__, 'cloudpickle=='+cloudpickle.__version__])\n",
    "\n",
    "        #Register & move the model in production\n",
    "        model_registered = mlflow.register_model(f'runs:/{run.info.run_id}/model', f\"{catalog}.{db}.{model_name}\")\n",
    "        client.set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=\"prod\", version=model_registered.version)\n",
    "    else:\n",
    "        print(f\"ERROR: couldn't access model for unknown reason - SDP pipeline will likely fail as model isn't available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1621d130-e52e-4dbd-8424-0290dff25e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if data_downloaded:\n",
    "    dbutils.notebook.exit(\"data downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f95dfd1-b2fc-4312-898d-7d5b2b15046a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "users data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "from collections import OrderedDict \n",
    "import uuid\n",
    "fake = Faker()\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "fake_firstname = F.udf(fake.first_name)\n",
    "fake_lastname = F.udf(fake.last_name)\n",
    "fake_email = F.udf(fake.ascii_company_email)\n",
    "\n",
    "def fake_date_between(months=0):\n",
    "  start = datetime.now() - timedelta(days=30*months)\n",
    "  return F.udf(lambda: fake.date_between_dates(date_start=start, date_end=start + timedelta(days=30)).strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "\n",
    "fake_date = F.udf(lambda:fake.date_time_this_month().strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_date_old = F.udf(lambda:fake.date_between_dates(date_start=datetime(2012,1,1), date_end=datetime(2015,12,31)).strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_address = F.udf(fake.address)\n",
    "canal = OrderedDict([(\"WEBAPP\", 0.5),(\"MOBILE\", 0.1),(\"PHONE\", 0.3),(None, 0.01)])\n",
    "fake_canal = F.udf(lambda:fake.random_elements(elements=canal, length=1)[0])\n",
    "fake_id = F.udf(lambda: str(uuid.uuid4()) if random.uniform(0, 1) < 0.98 else None)\n",
    "countries = ['FR', 'USA', 'SPAIN']\n",
    "fake_country = F.udf(lambda: countries[random.randint(0,2)])\n",
    "\n",
    "def get_df(size, month):\n",
    "  df = spark.range(0, size).repartition(10)\n",
    "  df = df.withColumn(\"id\", fake_id())\n",
    "  df = df.withColumn(\"firstname\", fake_firstname())\n",
    "  df = df.withColumn(\"lastname\", fake_lastname())\n",
    "  df = df.withColumn(\"email\", fake_email())\n",
    "  df = df.withColumn(\"address\", fake_address())\n",
    "  df = df.withColumn(\"canal\", fake_canal())\n",
    "  df = df.withColumn(\"country\", fake_country())  \n",
    "  df = df.withColumn(\"creation_date\", fake_date_between(month)())\n",
    "  df = df.withColumn(\"last_activity_date\", fake_date())\n",
    "  df = df.withColumn(\"gender\", F.round(F.rand()+0.2))\n",
    "  return df.withColumn(\"age_group\", F.round(F.rand()*10))\n",
    "\n",
    "df_customers = get_df(133, 12*30).withColumn(\"creation_date\", fake_date_old())\n",
    "for i in range(1, 24):\n",
    "  df_customers = df_customers.union(get_df(2000+i*200, 24-i))\n",
    "\n",
    "df_customers = df_customers.cache()\n",
    "\n",
    "ids = df_customers.select(\"id\").collect()\n",
    "ids = [r[\"id\"] for r in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "402a7f0e-350a-48d2-8523-ace52118e03a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Number of order per customer to generate a nicely distributed dataset\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "mu, sigma = 3, 2 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, int(len(ids)))\n",
    "s = [i if i > 0 else 0 for i in s]\n",
    "\n",
    "#Most of our customers have ~3 orders\n",
    "import matplotlib.pyplot as plt\n",
    "count, bins, ignored = plt.hist(s, 30, density=False)\n",
    "plt.show()\n",
    "s = [int(i) for i in s]\n",
    "\n",
    "order_user_ids = list()\n",
    "action_user_ids = list()\n",
    "for i, id in enumerate(ids):\n",
    "  for j in range(1, s[i]):\n",
    "    order_user_ids.append(id)\n",
    "    #Let's make 5 more actions per order (5 click on the website to buy something)\n",
    "    for j in range(1, 5):\n",
    "      action_user_ids.append(id)\n",
    "\n",
    "print(f\"Generated {len(order_user_ids)} orders and  {len(action_user_ids)} actions for {len(ids)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "777cdfb0-f9de-4b2e-ab88-cbcd050fd789",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "order data"
    }
   },
   "outputs": [],
   "source": [
    "orders = spark.createDataFrame([(i,) for i in order_user_ids], ['user_id'])\n",
    "orders = orders.withColumn(\"id\", fake_id())\n",
    "orders = orders.withColumn(\"transaction_date\", fake_date())\n",
    "orders = orders.withColumn(\"item_count\", F.round(F.rand()*2)+1)\n",
    "orders = orders.withColumn(\"amount\", F.col(\"item_count\")*F.round(F.rand()*30+10))\n",
    "orders = orders.cache()\n",
    "orders.repartition(10).write.format(\"json\").mode(\"overwrite\").save(folder+\"/orders\")\n",
    "cleanup_folder(folder+\"/orders\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7b466d-775c-4cc6-9274-15808b31123f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "website actions"
    }
   },
   "outputs": [],
   "source": [
    "#Website interaction\n",
    "import re\n",
    "\n",
    "platform = OrderedDict([(\"ios\", 0.5),(\"android\", 0.1),(\"other\", 0.3),(None, 0.01)])\n",
    "fake_platform = F.udf(lambda:fake.random_elements(elements=platform, length=1)[0])\n",
    "\n",
    "action_type = OrderedDict([(\"view\", 0.5),(\"log\", 0.1),(\"click\", 0.3),(None, 0.01)])\n",
    "fake_action = F.udf(lambda:fake.random_elements(elements=action_type, length=1)[0])\n",
    "fake_uri = F.udf(lambda:re.sub(r'https?:\\/\\/.*?\\/', \"https://databricks.com/\", fake.uri()))\n",
    "\n",
    "\n",
    "actions = spark.createDataFrame([(i,) for i in order_user_ids], ['user_id']).repartition(20)\n",
    "actions = actions.withColumn(\"event_id\", fake_id())\n",
    "actions = actions.withColumn(\"platform\", fake_platform())\n",
    "actions = actions.withColumn(\"date\", fake_date())\n",
    "actions = actions.withColumn(\"action\", fake_action())\n",
    "actions = actions.withColumn(\"session_id\", fake_id())\n",
    "actions = actions.withColumn(\"url\", fake_uri())\n",
    "actions = actions.cache()\n",
    "actions.write.format(\"csv\").option(\"header\", True).mode(\"overwrite\").save(folder+\"/events\")\n",
    "cleanup_folder(folder+\"/events\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21cc61b5-0450-41d4-985d-94a41d244dd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compute churn and save users"
    }
   },
   "outputs": [],
   "source": [
    "#Let's generate the Churn information. We'll fake it based on the existing data & let our ML model learn it\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "churn_proba_action = actions.groupBy('user_id').agg({'platform': 'first', '*': 'count'}).withColumnRenamed(\"count(1)\", \"action_count\")\n",
    "#Let's count how many order we have per customer.\n",
    "churn_proba = orders.groupBy('user_id').agg({'item_count': 'sum', '*': 'count'})\n",
    "churn_proba = churn_proba.join(churn_proba_action, ['user_id'])\n",
    "churn_proba = churn_proba.join(df_customers, churn_proba.user_id == df_customers.id)\n",
    "\n",
    "#Customer having > 5 orders are likely to churn\n",
    "churn_proba = (churn_proba.withColumn(\"churn_proba\", 5 +  F.when(((col(\"count(1)\") >=5) & (col(\"first(platform)\") == \"ios\")) |\n",
    "                                                                 ((col(\"count(1)\") ==3) & (col(\"gender\") == 0)) |\n",
    "                                                                 ((col(\"count(1)\") ==2) & (col(\"gender\") == 1) & (col(\"age_group\") <= 3)) |\n",
    "                                                                 ((col(\"sum(item_count)\") <=1) & (col(\"first(platform)\") == \"android\")) |\n",
    "                                                                 ((col(\"sum(item_count)\") >=10) & (col(\"first(platform)\") == \"ios\")) |\n",
    "                                                                 (col(\"action_count\") >=4) |\n",
    "                                                                 (col(\"country\") == \"USA\") |\n",
    "                                                                 ((F.datediff(F.current_timestamp(), col(\"creation_date\")) >= 90)) |\n",
    "                                                                 ((col(\"age_group\") >= 7) & (col(\"gender\") == 0)) |\n",
    "                                                                 ((col(\"age_group\") <= 2) & (col(\"gender\") == 1)), 80).otherwise(20)))\n",
    "\n",
    "churn_proba = churn_proba.withColumn(\"churn\", F.rand()*100 < col(\"churn_proba\"))\n",
    "churn_proba = churn_proba.drop(\"user_id\", \"churn_proba\", \"sum(item_count)\", \"count(1)\", \"first(platform)\", \"action_count\")\n",
    "churn_proba.repartition(100).write.format(\"json\").mode(\"overwrite\").save(folder+\"/users\")\n",
    "cleanup_folder(folder+\"/users\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01-load-data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
